{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from time import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import find\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "11314\n",
      "7532\n",
      "done in 1.976s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=False, random_state=1, subset='train',\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "print(len(dataset.data))\n",
    "data_samples = dataset.data\n",
    "n_docs = len(data_samples)\n",
    "\n",
    "dataset_test = fetch_20newsgroups(shuffle=False, random_state=1, subset='test',\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "print(len(dataset_test.data))\n",
    "data_test = dataset_test.data\n",
    "n_docs_test = len(data_test)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize documents and get Count Matrix\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 1.616s.\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_words,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_test = tf_vectorizer.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(I, J, K) = find(tf)\n",
    "\n",
    "docs_idxs = [[] for _ in range(n_docs)]\n",
    "docs_cnts = [[] for _ in range(n_docs)]\n",
    "\n",
    "for r, c, n in zip(I, J, K) :\n",
    "    docs_idxs[r].append(c)\n",
    "    docs_cnts[r].append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(I, J, K) = find(tf_test)\n",
    "\n",
    "docs_idxs_test = [[] for _ in range(n_docs_test)]\n",
    "docs_cnts_test = [[] for _ in range(n_docs_test)]\n",
    "\n",
    "for r, c, n in zip(I, J, K) :\n",
    "    docs_idxs_test[r].append(c)\n",
    "    docs_cnts_test[r].append(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Parameters\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "beta = 0.01\n",
    "offset = 2\n",
    "kappa = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from scipy.special import psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(word_list, phi, nums=False, tnum=10) :\n",
    "    max_args = np.argsort(phi, axis=1)[:, -tnum:] \n",
    "    words = []\n",
    "    for t in range(n_topics) :\n",
    "        words.append([i if nums else word_list[i] for i in max_args[t]])\n",
    "    return words\n",
    "\n",
    "word_list = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gammaln, logsumexp\n",
    "\n",
    "def perp(lambda_zt, didxs, dcnts) :\n",
    "    score = 0\n",
    "    index = [i for i in range(len(didxs)) if len(didxs[i]) > 0]\n",
    "    didxs = [didxs[i] for i in index]\n",
    "    dcnts = [dcnts[i] for i in index]\n",
    "    \n",
    "    n_docs = len(didxs)\n",
    "    lambda_hat, gamma = update_doc(range(n_docs), lambda_zt, didxs, dcnts)\n",
    "    \n",
    "    lambda_psi = psi(lambda_zt) - psi(lambda_zt.sum(1))[:, None]\n",
    "    gamma_psi = psi(gamma) - psi(gamma.sum(1))[:, None]\n",
    "\n",
    "    Nd = 0\n",
    "    for j, d in enumerate(range(n_docs)) :\n",
    "        words = didxs[d]\n",
    "        cnts = dcnts[d]\n",
    "        \n",
    "        Nd += sum(cnts)\n",
    "        \n",
    "        e_theta = gamma_psi[j]\n",
    "        e_beta =  lambda_psi[:, words]\n",
    "\n",
    "        topic_assign = e_theta[:, None] + e_beta \n",
    "        topic_assign = logsumexp(topic_assign, axis=0)\n",
    "        score += np.dot(topic_assign , cnts)\n",
    "       \n",
    "    score += np.sum((alpha - gamma) * gamma_psi)\n",
    "    score += np.sum(gammaln(gamma) - gammaln(alpha))\n",
    "    score += np.sum(gammaln(alpha * n_topics) - gammaln(gamma.sum(1))[:, np.newaxis])\n",
    "    \n",
    "    score += np.sum((beta - lambda_zt) * lambda_psi)\n",
    "    score += np.sum(gammaln(lambda_zt) - gammaln(beta))\n",
    "    score += np.sum(gammaln(beta * n_topics) - gammaln(lambda_zt.sum(1))[:, np.newaxis])\n",
    "    \n",
    "    score /= Nd\n",
    "    return np.exp(-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_doc(docs, lambda_zt, docs_idxs, docs_cnts) :\n",
    "    lambda_psi = np.exp(psi(lambda_zt) - psi(lambda_zt.sum(1))[:, None])\n",
    "    \n",
    "    lambda_hat = np.zeros((n_topics, n_words))\n",
    "    gamma_ret = np.zeros((len(docs), n_topics))\n",
    "    for j, d in enumerate(docs) :\n",
    "        words = docs_idxs[d]\n",
    "        cnts = docs_cnts[d]\n",
    "        \n",
    "        if len(words) > 0 :\n",
    "            topic_assign = np.zeros((n_topics, len(words)))\n",
    "            gamma = np.random.gamma(100., 1/100., (n_topics))\n",
    "            e_beta =  lambda_psi[:, words] #(K, N_d)\n",
    "\n",
    "            for i in range(100) :  \n",
    "                prev_gamma = gamma\n",
    "                e_theta = np.exp(psi(gamma) - psi(gamma.sum()))\n",
    "\n",
    "                topic_assign = e_theta[:, None] * e_beta \n",
    "                topic_assign = topic_assign / topic_assign.sum(0)\n",
    "\n",
    "                gamma = alpha + np.dot(topic_assign , cnts)\n",
    "                if np.mean(np.abs(prev_gamma - gamma)) < 0.001 :\n",
    "                    break\n",
    "\n",
    "            gamma_ret[j, :] = gamma\n",
    "\n",
    "            lambda_hat[:, words] += (topic_assign * cnts)\n",
    "\n",
    "    return lambda_hat, gamma_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lambda_zt(docs, t, lambda_zt) :\n",
    "    lr_t = (t + offset) ** (-kappa)\n",
    "    lambda_hat, gamma_ret = update_doc(docs, lambda_zt, docs_idxs, docs_cnts) \n",
    "    lambda_hat = lambda_hat * n_docs/len(docs) + beta\n",
    "    lambda_zt = (1 - lr_t) * (lambda_zt) + lr_t * (lambda_hat)\n",
    "    \n",
    "    return lambda_zt\n",
    "\n",
    "def run_SVI(n_iters, bsize, nt=10) :    \n",
    "    global n_topics\n",
    "    \n",
    "    t = 0\n",
    "    n_topics = nt\n",
    "    lambda_zt = np.random.gamma(100., 1/100., (n_topics, n_words))\n",
    "    \n",
    "    perps = np.zeros(n_iters)\n",
    "    perps_test = np.zeros(n_iters)\n",
    "    top_words = []\n",
    "    \n",
    "    for i in tqdm_notebook(range(n_iters)) :\n",
    "        for d in range(0, n_docs, bsize) :\n",
    "            batch = range(d, min(d+bsize, n_docs))\n",
    "            lambda_zt = update_lambda_zt(batch, t, lambda_zt)\n",
    "            t += 1\n",
    "        top_words.append(get_top_words(word_list, lambda_zt, tnum=50))\n",
    "        perps[i] = perp(lambda_zt, docs_idxs, docs_cnts)\n",
    "        perps_test[i] = perp(lambda_zt, docs_idxs_test, docs_cnts_test)\n",
    "        \n",
    "    return lambda_zt, perps, perps_test, top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "lambda_zt, perps, perps_test, top_words = run_SVI(300, 32, nt=10)\n",
    "#pickle.dump([lambda_zt, perps, perps_test, top_words], open(\"svi-10-300.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "lambda_zt, perps, perps_test, top_words = pickle.load(open(\"svi-10-300.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(perps, label=\"Training Set\")\n",
    "plt.xlabel(\"Epochs\", fontsize=20)\n",
    "plt.ylabel(\"Perplexity\", fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"perpsvi.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Topic Distribution by True Topic\n",
    "====================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "didxs = docs_idxs\n",
    "dcnts = docs_cnts\n",
    "index = [i for i in range(len(didxs)) if len(didxs[i]) > 0]\n",
    "didxs = [didxs[i] for i in index]\n",
    "dcnts = [dcnts[i] for i in index]\n",
    "n_docs = len(didxs)\n",
    "lambda_hat, gamma = update_doc(range(n_docs), lambda_zt, didxs, dcnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = gamma / gamma.sum(1)[:, np.newaxis]\n",
    "gamma_max = gamma.argmax(1)\n",
    "maptopic = [0, 1, 1, 1, 1, 1, 2, 3, 3, 4, 4, 5, 1, 6, 7, 0, 8, 8, 8, 0]\n",
    "\n",
    "targets = dataset.target\n",
    "targets_filt = [targets[i] for i in index]\n",
    "targets_mapped = [maptopic[c] for c in targets_filt]\n",
    "\n",
    "countmat = np.zeros((9, 10))\n",
    "for a, b in zip(targets_mapped, gamma_max) :\n",
    "    countmat[a, b] += 1\n",
    "    \n",
    "countmat = countmat / countmat.sum(1)[:, None]\n",
    "\n",
    "sns.heatmap(countmat, annot=True, cmap=\"Greens\")\n",
    "plt.xlabel(\"Predicted Topics\")\n",
    "plt.ylabel(\"True Topics\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Changes in Word Distribution\n",
    "================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_topic = -5\n",
    "\n",
    "topic_words_list = []\n",
    "for t in range(10) :\n",
    "    nt = []\n",
    "    for i in range(len(top_words)) :\n",
    "        nt.append(top_words[i][t])\n",
    "    topic_words_list.append(nt)\n",
    "\n",
    "dict_words = {}\n",
    "for i in range(300) :\n",
    "    for j, word in enumerate(topic_words_list[selected_topic][i]) :\n",
    "        if word in dict_words :\n",
    "            dict_words[word][i] = j\n",
    "        else :\n",
    "            dict_words[word] = [-5]*300\n",
    "            dict_words[word][i] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "for word in dict_words :\n",
    "    plt.plot(dict_words[word], label=word)\n",
    "    plt.text(i + 1, dict_words[word][-1], word)\n",
    "plt.ylabel(r\"Rank of the word (according to $\\beta_{kv}$)\", fontsize=20)\n",
    "plt.xlabel(\"Epochs\", fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"svi-religion.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Top Words\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rights', 'armenians', 'war', 'israel', 'armenian', 'president', 'car', 'people', 'government', 'going']\n",
      "['encryption', '13', 'software', 'disk', 'ftp', '145', 'files', '25', 'key', 'file']\n",
      "['help', 'using', 've', 'thanks', 'windows', 'problem', 'need', 'like', 'does', 'use']\n",
      "['really', 'said', 'year', 'time', 'good', 'think', 'like', 'just', 'don', 'know']\n",
      "['16', 'db', '12', '11', '17', '20', '15', 'team', 'game', '10']\n",
      "['right', 'think', 'say', 'believe', 'does', 'make', 'way', 'did', 'god', 'people']\n",
      "['server', 'available', 'bible', 'university', 'list', 'send', 'information', 'mail', 'com', 'edu']\n",
      "['package', 'output', 'pc', 'image', 'info', 'data', 'card', '00', 'program', 'space']\n",
      "['bus', 'apple', 'mac', 'hard', 'driver', 'cx', 'power', 'screen', 'scsi', 'drive']\n",
      "['0t', '1t', '34u', '1d9', 'pl', 'a86', 'b8f', 'g9v', 'max', 'ax']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10) :\n",
    "    print(get_top_words(word_list, lambda_zt, tnum=10)[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Coherence\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence(top_words) :\n",
    "    tc = []\n",
    "    tfidf = TfidfTransformer().fit_transform(tf).todense()\n",
    "    for words in top_words :\n",
    "        tk = np.zeros((len(words), len(words)))\n",
    "        for i in range(len(words) - 1) :\n",
    "            for j in range(i + 1, len(words)) :\n",
    "                num = np.dot(tfidf[:, words[i]].T, tfidf[:, words[j]])[0, 0]\n",
    "                denom = np.sum(tfidf[:, words[i]])\n",
    "                tk[i, j] = np.log((num + 0.000001)/denom)\n",
    "                tk[j, i] = tk[i, j]\n",
    "        tc.append(tk)\n",
    "    return tc\n",
    "\n",
    "tp = get_top_words(word_list, lambda_zt, nums=True, tnum=1000)\n",
    "c = coherence(tp)\n",
    "pickle.dump(c, open(\"sv-10-300-cohenrence.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pickle.load(open(\"sv-10-300-cohenrence.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10) :\n",
    "    plt.imshow(c[-1][-100:, -100:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "for i in range(10) :\n",
    "    cscores = []\n",
    "    for j in range(2, 100) :\n",
    "        coh = c[i][-j:, -j:].sum()/(j * (j-1))\n",
    "        cscores.append(coh)\n",
    "    plt.plot(range(2, 100), cscores, label=i)\n",
    "    plt.ylabel(\"Coherence Score\", fontsize=20)\n",
    "plt.xlabel(\"Epochs\", fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(loc='upper right', ncol=4, fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pickle.load(open(\"sv-10-300-cohenrence.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsort = np.sort(lambda_zt, axis=1)\n",
    "lsort = lsort / lsort.sum(1)[:, None]\n",
    "for i in range(10) :\n",
    "    coh = c[i][-20:, -20:].sum(0)/(20)\n",
    "    plt.scatter(lsort[i, -20:], coh, label=i, s=10)\n",
    "    \n",
    "plt.xlim(0.0, 0.1)\n",
    "plt.ylabel(\"Coherence Score\", fontsize=20)\n",
    "plt.xlabel(r\"Probability of word in topic $\\beta_{kv}$\", fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(loc='lower right', ncol=3, fontsize=12, title=\"k\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cohvsprobfull_svi.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsort = np.sort(lambda_zt, axis=1)\n",
    "lsort = lsort / lsort.sum(1)[:, None]\n",
    "for i in range(10) :\n",
    "    coh = c[i][-4:, -50:-4].sum(0)/4\n",
    "    plt.scatter(lsort[i, -50:-4], coh, s=10, label=i)\n",
    "    \n",
    "plt.ylabel(\"Coherence Score\", fontsize=20)\n",
    "plt.xlabel(r\"Probability of word in topic $\\beta_{kv}$\", fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(loc='lower right', ncol=2, fontsize=12, title=\"k\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cohvsprob3_svi.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Perplexity by Number of Topics\n",
    "==================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perp_by_ntopics = {}\n",
    "perp_by_ntopics_test = {}\n",
    "for nt in tqdm_notebook(range(2, 30, 3)) :\n",
    "    lambda_zt, perps, perps_test, top_words = run_SVI(100, 32, nt=nt)\n",
    "    perp_by_ntopics[nt] = perps\n",
    "    perp_by_ntopics_test[nt] = perps_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([perp_by_ntopics, perp_by_ntopics_test], open(\"svi-perps.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "perp_by_ntopics, perp_by_ntopics_test = pickle.load(open(\"svi-perps.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(perp_by_ntopics.keys(), [x[-1] for x in perp_by_ntopics.values()])\n",
    "plt.xlabel(\"Number of Topics\", fontsize=20)\n",
    "plt.ylabel(\"Final Perplexity\", fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"perpbytopic_svi.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(perp_by_ntopics_test.keys(), [x[-1] for x in perp_by_ntopics_test.values()])\n",
    "#plt.plot(perp_by_ntopics.keys(), [x[-1] for x in perp_by_ntopics.values()])\n",
    "plt.xlabel(\"Number of Topics\", fontsize=20)\n",
    "plt.ylabel(\"Final Perplexity\", fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"perpbytopic_test_svi.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
