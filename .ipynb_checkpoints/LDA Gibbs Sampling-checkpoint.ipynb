{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_docs = 2000\n",
    "n_words = 2000\n",
    "n_topics = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.278s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data #[:n_docs]\n",
    "n_docs = len(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize documents and get Count Matrix\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 1.553s.\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_words,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(I, J, K) = find(tf)\n",
    "\n",
    "docs = [[] for _ in range(n_docs)]\n",
    "for r, c, n in zip(I, J, K) :\n",
    "    docs[r] += [c for i in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Counts\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dz = np.zeros((n_docs, n_topics))\n",
    "n_d = np.zeros((n_docs))\n",
    "n_zt = np.zeros((n_topics, n_words))\n",
    "n_z = np.zeros((n_topics))\n",
    "topic_assign = [[0 for _ in range(len(doc))] for doc in docs]\n",
    "\n",
    "for d in range(n_docs) :\n",
    "    for i, w in enumerate(docs[d]):\n",
    "        z = np.random.randint(n_topics)\n",
    "        topic_assign[d][i] = z\n",
    "        n_dz[d, z] += 1\n",
    "        n_d[d] += 1\n",
    "        n_zt[z, w] += 1\n",
    "        n_z[z] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Parameters\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "beta = 0.01\n",
    "\n",
    "\n",
    "theta = np.zeros((n_docs, n_topics))\n",
    "phi = np.zeros((n_topics, n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_topic(d, w, i) :\n",
    "    zc = topic_assign[d][i]\n",
    "        \n",
    "    n_dz[d, zc] -= 1\n",
    "    n_d[d] -= 1\n",
    "    n_zt[zc, w] -= 1\n",
    "    n_z[zc] -= 1\n",
    "            \n",
    "    a = (n_zt[:, w] + beta) / (n_z + beta * n_words)\n",
    "    b = (n_dz[d, :] + alpha) / (n_d[d] + n_topics * alpha)\n",
    "\n",
    "    pz = a*b\n",
    "    pz /= sum(pz)\n",
    "    \n",
    "    z = np.random.choice(range(n_topics), p=pz)\n",
    "    \n",
    "    n_dz[d, z] += 1\n",
    "    n_d[d] += 1\n",
    "    n_zt[z, w] += 1\n",
    "    n_z[z] += 1\n",
    "    topic_assign[d][i] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gibbs(n_iters) :\n",
    "    for i in tqdm_notebook(range(n_iters)) :\n",
    "        for doc in range(n_docs) :\n",
    "            for i, word in enumerate(docs[doc]) :\n",
    "                assign_topic(doc, word, i)\n",
    "                \n",
    "    phi = (n_zt + beta) / (n_zt.sum(1)[:, None] + beta*n_words)\n",
    "    theta = (n_dz + alpha) / (n_dz.sum(1)[:, None] + alpha*n_topics)\n",
    "    return phi, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(word_list, phi) :\n",
    "    max_args = np.argsort(phi, axis=1)[:, -10:] \n",
    "    for t in range(n_topics) :\n",
    "        print([word_list[i] for i in max_args[t]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcbcb10e726a439bba59d416a5901199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['13', '16', '20', '14', '11', '12', '15', '25', '10', '00']\n",
      "['armenia', 'turkey', 'government', 'world', 'jews', 'war', 'armenians', 'turkish', 'people', 'armenian']\n",
      "['0t', '1t', '1d9', '145', 'pl', 'a86', 'b8f', 'g9v', 'max', 'ax']\n",
      "['rights', 'think', 'don', 'israel', 'state', 'law', 'right', 'government', 'gun', 'people']\n",
      "['work', 've', 'time', 'does', 'know', 'good', 'just', 'like', 'use', 'don']\n",
      "['say', 'time', 'did', 'like', 'didn', 'don', 'know', 'just', 'said', 'people']\n",
      "['mac', 'windows', 'drives', 'hard', 'use', 'bit', 'disk', 'card', 'scsi', 'drive']\n",
      "['like', 'stephanopoulos', 'people', 'just', 'going', 'don', 'president', 'think', 'mr', 'know']\n",
      "['bike', 'time', 've', 'know', 'new', 'good', 'don', 'car', 'just', 'like']\n",
      "['mv', '34u', 'c_', 'bh', 'lk', 'chz', 'ah', 'w7', 'cx', 'db']\n",
      "['display', 'problem', 'application', 'windows', 'set', 'program', 'widget', 'using', 'use', 'window']\n",
      "['just', 'don', 'bible', 'say', 'believe', 'think', 'does', 'people', 'jesus', 'god']\n",
      "['ripem', 'faq', 'file', 'des', 'gif', 'use', 'bit', 'public', 'jpeg', 'key']\n",
      "['keys', 'use', 'government', 'output', 'clipper', 'entry', 'file', 'encryption', 'chip', 'key']\n",
      "['does', 'water', 've', 'think', 'time', 'know', 'good', 'like', 'just', 'don']\n",
      "['data', 'version', 'program', 'pc', 'use', 'graphics', 'dos', 'image', 'windows', 'software']\n",
      "['program', 'orbit', 'shuttle', 'satellite', 'science', 'earth', 'data', 'launch', 'nasa', 'space']\n",
      "['information', 'anonymous', 'mail', 'list', 'available', 'pub', 'ftp', 'file', 'com', 'edu']\n",
      "['1992', 'health', 'university', 'states', 'information', 'national', 'april', 'year', '1993', 'new']\n",
      "['good', 'think', 'hockey', 'players', 'play', 'season', 'games', 'year', 'team', 'game']\n"
     ]
    }
   ],
   "source": [
    "phi, theta = run_gibbs(100)\n",
    "word_list = tf_vectorizer.get_feature_names()\n",
    "get_top_words(word_list, phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
