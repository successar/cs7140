{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = 2000\n",
    "n_topics = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.189s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "data_samples = dataset.data #[:n_docs]\n",
    "n_docs = len(data_samples)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize documents and get Count Matrix\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 1.530s.\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_words,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(I, J, K) = find(tf)\n",
    "\n",
    "docs_idxs = [[] for _ in range(n_docs)]\n",
    "docs_cnts = [[] for _ in range(n_docs)]\n",
    "\n",
    "for r, c, n in zip(I, J, K) :\n",
    "    docs_idxs[r].append(c)\n",
    "    docs_cnts[r].append(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Parameters\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "beta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from scipy.special import psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_doc(docs, lambda_zt) :\n",
    "    lambda_psi = np.exp(psi(lambda_zt) - psi(lambda_zt.sum(1))[:, None])\n",
    "    \n",
    "    lambda_hat = np.zeros((n_topics, n_words))\n",
    "    for d in docs :\n",
    "        words = docs_idxs[d]\n",
    "        cnts = docs_cnts[d]\n",
    "        \n",
    "        if len(words) > 0:\n",
    "            topic_assign = np.zeros((n_topics, len(words)))\n",
    "            gamma = np.random.gamma(100., 1/100., (n_topics))\n",
    "            e_beta =  lambda_psi[:, words] #(K, N_d)\n",
    "            \n",
    "            for i in range(100) :  \n",
    "                prev_gamma = gamma\n",
    "                e_theta = np.exp(psi(gamma) - psi(gamma.sum()))\n",
    "\n",
    "                topic_assign = e_theta[:, None] * e_beta \n",
    "                topic_assign = topic_assign / topic_assign.sum(0)\n",
    "\n",
    "                gamma = alpha + np.dot(topic_assign , cnts)\n",
    "                if np.mean(np.abs(prev_gamma - gamma)) < 0.001 :\n",
    "                    break\n",
    "\n",
    "            lambda_hat[:, words] += (topic_assign * cnts)\n",
    "\n",
    "    return lambda_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 2\n",
    "kappa = 0.75\n",
    "\n",
    "def update_lambda_zt(docs, t, lambda_zt) :\n",
    "    lr_t = (t + offset) ** (-kappa)\n",
    "    lambda_hat = update_doc(docs, lambda_zt) \n",
    "    lambda_hat = lambda_hat * n_docs/len(docs) + beta\n",
    "    lambda_zt = (1 - lr_t) * (lambda_zt) + lr_t * (lambda_hat)\n",
    "    \n",
    "    return lambda_zt\n",
    "\n",
    "def run_SVI(n_iters, bsize) :    \n",
    "    t = 0\n",
    "    lambda_zt = np.random.gamma(100., 1/100., (n_topics, n_words))\n",
    "    \n",
    "    for i in tqdm_notebook(range(n_iters)) :\n",
    "        for d in range(0, n_docs, bsize) :\n",
    "            batch = range(d, min(d+bsize, n_docs))\n",
    "            lambda_zt = update_lambda_zt(batch, t, lambda_zt)\n",
    "            t += 1\n",
    "            \n",
    "    return lambda_zt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(word_list, phi) :\n",
    "    max_args = np.argsort(phi, axis=1)[:, -10:] \n",
    "    for t in range(n_topics) :\n",
    "        print([word_list[i] for i in max_args[t]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c674e8885c3448c9f28801ff87e5bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lambda_zt = run_SVI(100, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['true', 'home', 'life', 'say', 'does', 'people', 'believe', 'pl', 'jesus', 'god']\n",
      "['bus', 'drives', 'hard', 'faith', 'disk', 'db', 'scsi', 'card', 'drive', 'max']\n",
      "['policy', 'number', 'research', 'stop', 'hockey', 'points', 'center', 'sort', 'bad', 'israel']\n",
      "['anti', 'israeli', 'killed', 'children', 'turkish', 'armenians', 'jews', 'armenian', 'said', 'people']\n",
      "['effect', 'data', 'mode', 'use', 'clipper', 'keys', 'encryption', 'bit', 'chip', 'key']\n",
      "['la', 'st', 'cx', 'players', 'vs', 'period', '28', '26', 'season', 'game']\n",
      "['road', 'water', 'bike', 'april', 'health', 'university', 'value', 'american', 'states', 'new']\n",
      "['right', 'didn', 've', 'going', 'like', 'just', 'know', 'people', 'think', 'don']\n",
      "['data', 'use', 'program', 'version', 'graphics', 'software', 'available', 'code', 'window', 'image']\n",
      "['heard', 'won', 'old', 'little', 'new', 'actually', 'time', 'probably', 'good', 'year']\n",
      "['files', 'list', 'information', 'send', 'email', 'ftp', 'mail', 'com', 'file', 'edu']\n",
      "['try', 'let', 'know', 'say', 'time', 'really', 'does', 'good', 'like', 'just']\n",
      "['11', '16', '14', '12', '20', '25', 'mr', '15', '00', '10']\n",
      "['cost', 'process', 'high', 'data', 'couple', 'size', 'hardware', 'power', 'use', 'space']\n",
      "['using', 'pc', 'use', 'help', 'know', 'dos', 'does', 'problem', 'thanks', 'windows']\n",
      "['use', 'best', 'guns', 'speed', 'apple', 'need', 'driver', 'line', 'better', 've']\n",
      "['war', 'win', 'time', 'play', 'national', 'better', 'said', 'team', 'years', 'did']\n",
      "['quality', 'according', 'ah', 'exist', 'matter', 'live', 'feel', 'thing', 'left', '145']\n",
      "['3t', 'bhj', '1t', '0t', '34u', '1d9', 'a86', 'b8f', 'g9v', 'ax']\n",
      "['people', 'use', 'control', 'state', 'security', 'public', 'president', 'gun', 'government', 'law']\n"
     ]
    }
   ],
   "source": [
    "word_list = tf_vectorizer.get_feature_names()\n",
    "get_top_words(word_list, lambda_zt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
