{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from time import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import find\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "11314\n",
      "7532\n",
      "done in 1.918s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=False, random_state=1, subset='train',\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "print(len(dataset.data))\n",
    "data_samples = dataset.data\n",
    "n_docs = len(data_samples)\n",
    "\n",
    "dataset_test = fetch_20newsgroups(shuffle=False, random_state=1, subset='test',\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "print(len(dataset_test.data))\n",
    "data_test = dataset_test.data\n",
    "n_docs_test = len(data_test)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize documents and get Count Matrix\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 2.197s.\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_words,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "tf_test = tf_vectorizer.transform(data_test)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(I, J, K) = find(tf)\n",
    "\n",
    "docs = [[] for _ in range(n_docs)]\n",
    "for r, c, n in zip(I, J, K) :\n",
    "    docs[r] += [c for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(I, J, K) = find(tf_test)\n",
    "\n",
    "docs_test = [[] for _ in range(n_docs_test)]\n",
    "for r, c, n in zip(I, J, K) :\n",
    "    docs_test[r] += [c for i in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Parameters\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "beta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(word_list, phi, nums=False, tnum=10) :\n",
    "    max_args = np.argsort(phi, axis=1)[:, -tnum:] \n",
    "    words = []\n",
    "    for t in range(n_topics) :\n",
    "        words.append([i if nums else word_list[i] for i in max_args[t]])\n",
    "    return words\n",
    "\n",
    "word_list = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_topic(d, w, i) :\n",
    "    zc = topic_assign[d][i]\n",
    "        \n",
    "    n_dz[d, zc] -= 1\n",
    "    n_d[d] -= 1\n",
    "    n_zt[zc, w] -= 1\n",
    "    n_z[zc] -= 1\n",
    "            \n",
    "    a = (n_zt[:, w] + beta) / (n_z + beta * n_words)\n",
    "    b = (n_dz[d, :] + alpha) / (n_d[d] + n_topics * alpha)\n",
    "\n",
    "    pz = a*b\n",
    "    pz /= sum(pz)\n",
    "    \n",
    "    z = np.random.choice(range(n_topics), p=pz)\n",
    "    \n",
    "    n_dz[d, z] += 1\n",
    "    n_d[d] += 1\n",
    "    n_zt[z, w] += 1\n",
    "    n_z[z] += 1\n",
    "    topic_assign[d][i] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gibbs(n_iters) :\n",
    "    perps = []\n",
    "    top_words = []\n",
    "    for i in tqdm_notebook(range(n_iters)) :\n",
    "        for doc in range(n_docs) :\n",
    "            for i, word in enumerate(docs[doc]) :\n",
    "                assign_topic(doc, word, i)\n",
    "                \n",
    "        phi = (n_zt + beta) / (n_zt.sum(1)[:, None] + beta*n_words)\n",
    "        theta = (n_dz + alpha) / (n_dz.sum(1)[:, None] + alpha*n_topics)\n",
    "        top_words.append(get_top_words(word_list, phi, tnum=50))\n",
    "        \n",
    "        score = np.log(np.matmul(theta, phi))\n",
    "        score = -tf.multiply(score).sum()\n",
    "        score = score / tf.sum()\n",
    "        perps.append(score)\n",
    "        \n",
    "    return phi, theta, perps, top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "\n",
    "n_dz = np.zeros((n_docs, n_topics))\n",
    "n_d = np.zeros((n_docs))\n",
    "n_zt = np.zeros((n_topics, n_words))\n",
    "n_z = np.zeros((n_topics))\n",
    "topic_assign = [[0 for _ in range(len(doc))] for doc in docs]\n",
    "\n",
    "for d in range(n_docs) :\n",
    "    for i, w in enumerate(docs[d]):\n",
    "        z = np.random.randint(n_topics)\n",
    "        topic_assign[d][i] = z\n",
    "        n_dz[d, z] += 1\n",
    "        n_d[d] += 1\n",
    "        n_zt[z, w] += 1\n",
    "        n_z[z] += 1\n",
    "        \n",
    "phi, theta, perps, top_words = run_gibbs(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([phi, theta, perps, top_words], open(\"lda-gibbs-10-300.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi, theta, perps, top_words = pickle.load(open(\"lda-gibbs-10-300.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([np.exp(x) for x in perps], label=\"Training Set\")\n",
    "plt.xlabel(\"Epochs\", fontsize=20)\n",
    "plt.ylabel(\"Perplexity\", fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"perpgibbs.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_topic = -3\n",
    "\n",
    "topic_words_list = []\n",
    "for t in range(10) :\n",
    "    nt = []\n",
    "    for i in range(len(top_words)) :\n",
    "        nt.append(top_words[i][t])\n",
    "    topic_words_list.append(nt)\n",
    "\n",
    "dict_words = {}\n",
    "for i in range(300) :\n",
    "    for j, word in enumerate(topic_words_list[selected_topic][i]) :\n",
    "        if word in dict_words :\n",
    "            dict_words[word][i] = j\n",
    "        else :\n",
    "            dict_words[word] = [-5]*300\n",
    "            dict_words[word][i] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "for word in dict_words :\n",
    "    plt.plot(dict_words[word], label=word)\n",
    "    plt.text(i + 1, dict_words[word][-1], word)\n",
    "plt.ylabel(r\"Rank of the word (according to $\\beta_{kv}$)\", fontsize=20)\n",
    "plt.xlabel(\"Epochs\", fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"gibbs-electronic.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dos', 'know', 'thanks', 'disk', 'does', 'like', 'scsi', 'use', 'card', 'drive']\n",
      "['new', 'research', 'information', 'nasa', 'chip', 'encryption', 'use', 'data', 'space', 'key']\n",
      "['game', 'car', 'year', 'time', 'know', 'think', 'good', 'don', 'like', 'just']\n",
      "['did', 'jews', 'president', 'turkish', 'armenians', 'israel', 'mr', 'armenian', 'said', 'people']\n",
      "['believe', 'know', 'say', 'just', 'jesus', 'don', 'think', 'does', 'people', 'god']\n",
      "['1t', '34u', '1d9', '145', 'pl', 'a86', 'b8f', 'g9v', 'max', 'ax']\n",
      "['14', '16', '17', '12', '20', '25', '15', '11', '10', '00']\n",
      "['version', 'files', 'available', 'program', 'db', 'image', 'file', 'windows', 'window', 'use']\n",
      "['internet', 'address', 'list', 'email', 'information', 'send', 'mail', 'file', 'com', 'edu']\n",
      "['state', 'just', 'like', 'make', 'right', 'gun', 'think', 'government', 'don', 'people']\n"
     ]
    }
   ],
   "source": [
    "n_topics = 10\n",
    "for i in range(n_topics) :\n",
    "    print(get_top_words(word_list, phi, tnum=10)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([np.exp(x) for x in perps])\n",
    "plt.xlabel(\"Iteration\", fontsize=20)\n",
    "plt.ylabel(\"Perplexity\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64852f7e0ad544a1b8d48c37a5e8ad43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531ea969016448bf897312bb9cc108ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be13b92976954ea1a512bec9a0cb4925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8dbfe1536da451290b3ef1d015756c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf79de2332e042a08a80b3e250b923f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122ab3bfcb1d4276bb6f20e1d223c252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e375097d4c3e4f649f0e9e0966649cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f5a350fee043fa8745400ac1a33652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43be605e8c4047f2b092a5e6a010df9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32071f873dd94339a299e7a87675f570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53c8af5d74741f39dc0538052d3429a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def coherence(top_words) :\n",
    "    tc = []\n",
    "    tfidf = TfidfTransformer().fit_transform(tf).todense()\n",
    "    for words in tqdm_notebook(top_words) :\n",
    "        tk = np.zeros((len(words), len(words)))\n",
    "        for i in tqdm_notebook(range(len(words) - 1)) :\n",
    "            for j in range(i + 1, len(words)) :\n",
    "                num = np.dot(tfidf[:, words[i]].T, tfidf[:, words[j]])[0, 0]\n",
    "                denom = np.sum(tfidf[:, words[i]])\n",
    "                tk[i, j] = np.log((num + 0.000001)/denom)\n",
    "                tk[j, i] = tk[i, j]\n",
    "        tc.append(tk)\n",
    "    return tc\n",
    "\n",
    "tp = get_top_words(word_list, phi, nums=True, tnum=1000)\n",
    "c = coherence(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "for i in range(10) :\n",
    "    cscores = []\n",
    "    for j in range(2, 100) :\n",
    "        coh = c[i][-j:, -j:].sum()/(j * (j-1))\n",
    "        cscores.append(coh)\n",
    "    plt.plot(range(2, 100), cscores, label=i)\n",
    "plt.ylabel(\"Coherence Score\", fontsize=20)\n",
    "plt.xlabel(\"Epochs\", fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "#plt.legend(fontsize=12)\n",
    "plt.legend(loc='upper right', ncol=4, fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cohbyntop_gibbs.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsort = np.sort(phi, axis=1)\n",
    "lsort = lsort / lsort.sum(1)[:, None]\n",
    "for i in range(10) :\n",
    "    coh = c[i][-20:, -20:].sum(0)/20\n",
    "    plt.scatter(lsort[i, -20:], coh, label=i, s=10)\n",
    "    \n",
    "plt.xlim(0.0, 0.1)\n",
    "plt.ylabel(\"Coherence Score\", fontsize=20)\n",
    "plt.xlabel(r\"Probability of word in topic $\\beta_{kv}$\", fontsize=20)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(loc='lower right', ncol=3, fontsize=12, title=\"k\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cohvsprobfull_gibbs.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsort = np.sort(phi, axis=1)\n",
    "lsort = lsort / lsort.sum(1)[:, None]\n",
    "for i in range(10) :\n",
    "    coh = c[i][-4:, -50:-4].sum(0)/4\n",
    "    plt.scatter(lsort[i, -50:-4], coh, s=10, label=i)\n",
    "    \n",
    "plt.xlim(0.0, 0.021)\n",
    "\n",
    "plt.ylabel(\"Coherence Score\", fontsize=20)\n",
    "plt.xlabel(r\"Probability of word in topic $\\beta_{kv}$\", fontsize=15)\n",
    "plt.xticks(fontsize=18, rotation=90)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.legend(loc='lower right', ncol=2, fontsize=12, title=\"k\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cohvsprob3_gibbs.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
